# Phase 0 — Groundwork & Spec

## What I built today
- `docs/spec.md` — Defined the Phase 0 product requirements and success metrics.
- `docs/metrics_glossary.md` — Catalogued core GPU performance counters with units and dependencies.
- `docs/architecture_overview.md` — Sketched the target architecture and module plan.
- `schema/run.schema.json` — Authored strict JSON Schema for profiling runs.
- `samples/trace_minimal.json` — Crafted a minimal schema-compliant run trace.
- `tests/test_schema_validation.py` — Added pytest coverage for valid and invalid traces.
- `jsonschema/` — Shipped a lightweight offline validator to enforce the schema without external installs.
- `pyproject.toml` — Configured packaging metadata and dev dependencies.
- `README.md` — Documented setup, testing, and validation instructions.
- `LICENSE` & `.gitignore` — Added licensing and repository hygiene files.

## Decisions & Rationale
- Enforced `additionalProperties: false` to guarantee adapters stay aligned with the schema.
- Chose enumerations for `software.backend` and `precision` to constrain early integrations.
- Required kernel-level occupancy and memory counters to support arithmetic intensity and bottleneck analysis later.
- Stored sample trace in JSON (vs Parquet) to keep onboarding lightweight while planning Parquet parity.
- Embedded a slim validator to sidestep network installs while keeping the API identical to `jsonschema`.

## Gaps / Open Questions
- Need confirmation on preferred fingerprinting algorithm (hash vs. statistical signature).
- Determine whether percentile metrics should be mandatory once histogram data is available.
- Clarify if multi-GPU runs require per-device breakdowns in Phase 1 or later.

## Next Steps (for Phase 1)
- Implement ONNX Runtime adapter to emit per-op latency timelines into `core/profiler.py`.
- Add CLI entrypoint (`cli/profile.py`) that executes a model under ORT with profiling enabled.
- Ensure the adapter writes `run.json` artifacts that pass `schema/run.schema.json` validation.
- Introduce regression tests covering multiple ops and verifying summary aggregation logic.

<!-- NEXT_PROMPT_HINT:
Phase=1
Goal=Implement ONNX Runtime per-op latency profiler and emit run.json validating against schema/run.schema.json
ArtifactsToModify=core/profiler.py, cli/profile.py
-->
