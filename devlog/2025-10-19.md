# Phase 0 — Groundwork & Spec

## What I built today
- `docs/spec.md` — Defined the Phase 0 product requirements and success metrics.
- `docs/metrics_glossary.md` — Catalogued core GPU performance counters with units and dependencies.
- `docs/architecture_overview.md` — Sketched the target architecture and module plan.
- `schema/run.schema.json` — Authored strict JSON Schema for profiling runs.
- `samples/trace_minimal.json` — Crafted a minimal schema-compliant run trace.
- `tests/test_schema_validation.py` — Added pytest coverage for valid and invalid traces.
- `jsonschema/` — Shipped a lightweight offline validator to enforce the schema without external installs.
- `pyproject.toml` — Configured packaging metadata and dev dependencies.
- `README.md` — Documented setup, testing, and validation instructions.
- `LICENSE` & `.gitignore` — Added licensing and repository hygiene files.

## Decisions & Rationale
- Enforced `additionalProperties: false` to guarantee adapters stay aligned with the schema.
- Chose enumerations for `software.backend` and `precision` to constrain early integrations.
- Required kernel-level occupancy and memory counters to support arithmetic intensity and bottleneck analysis later.
- Stored sample trace in JSON (vs Parquet) to keep onboarding lightweight while planning Parquet parity.
- Embedded a slim validator to sidestep network installs while keeping the API identical to `jsonschema`.

## Gaps / Open Questions
- Need confirmation on preferred fingerprinting algorithm (hash vs. statistical signature).
- Determine whether percentile metrics should be mandatory once histogram data is available.
- Clarify if multi-GPU runs require per-device breakdowns in Phase 1 or later.

## Next Steps (for Phase 1)
- Implement ONNX Runtime adapter to emit per-op latency timelines into `core/profiler.py`.
- Add CLI entrypoint (`cli/profile.py`) that executes a model under ORT with profiling enabled.
- Ensure the adapter writes `run.json` artifacts that pass `schema/run.schema.json` validation.
- Introduce regression tests covering multiple ops and verifying summary aggregation logic.

<!-- NEXT_PROMPT_HINT:
Phase=1
Goal=Implement ONNX Runtime per-op latency profiler and emit run.json validating against schema/run.schema.json
ArtifactsToModify=core/profiler.py, cli/profile.py
-->

# Phase 1 — Core Profiler Engine (ONNX Runtime)
## What I built today
- Implemented per-op ONNX Runtime adapter → `adapters/onnxrt_adapter.py`
- Integrated `core/profiler.py` for orchestration + schema validation
- Added CLI entrypoint `cli/profile.py` using Typer
- Added tests for schema compliance and summary logic

## Decisions & Rationale
- Stubbed kernel metrics with zeros while enforcing schema-required fields for future GPU integrations
- Allowed dependency injection for ONNX Runtime sessions to keep tests hermetic without network installs
- Validated artifacts before writing to disk to avoid leaking broken traces downstream

## Gaps / Open Questions
- How to integrate Nsight/CUPTI in Phase 2
- Whether to parallelize runs via asyncio

## Next Steps (Phase 2)
- Implement `torch_adapter.py` and `tensorrt_adapter.py`
- Add unified `Adapter` interface
- Build golden-trace comparison tests
<!-- NEXT_PROMPT_HINT:
Phase=2
Goal=Add multi-backend adapters (PyTorch, TensorRT) with uniform schema + comparison tests
ArtifactsToModify=adapters/torch_adapter.py, adapters/tensorrt_adapter.py, tests/test_adapters.py
-->

# Phase 2 — Multi-backend Adapters (PyTorch + TensorRT)
## What I built today
- Introduced `Adapter` interface and registry
- Implemented `torch_adapter.py` with PyTorch Profiler per-op timings
- Implemented `tensorrt_adapter.py` with graceful stub fallback
- Extended `core/profiler.py` and `cli/profile.py` to select backends
- Added schema parity + golden-trace tests; conditional skips

## Decisions & Rationale
- Unified schema across backends to keep fingerprints comparable
- Stubbed TRT to unblock users without local installation
- Kept counters minimal (latency only) to de-risk before CUPTI/Nsight integration

## Gaps / Open Questions
- How to map PyTorch op names to high-level types consistently
- When to add kernel-level counters (Phase 4+)

## Next Steps (Phase 3)
- Build fingerprint engine: hardware-normalized per-layer vectors
- Implement similarity + diff across runs
- Add CLI: `neurolens fingerprint` and `neurolens compare`
<!-- NEXT_PROMPT_HINT:
Phase=3
Goal=Implement fingerprint builder + similarity/diff; generate stable layer-wise performance fingerprints and a compare CLI
ArtifactsToModify=fingerprint/builder.py,fingerprint/similarity.py,cli/fingerprint.py,cli/compare.py,tests/test_fingerprint.py
-->

# Hotfix — Remove binaries and generate models on the fly
## What I changed
- Removed committed .onnx/.pt files and added ignore rules
- Added tools/gen_add_onnx.py and tools/gen_tiny_linear.py
- Tests now generate models into tmp_path and skip gracefully when deps missing
## Why
- Codex PRs cannot include binary files; reproducible generation keeps PRs lightweight and CI-friendly
## Next
- Proceed with Phase 2 PR; then start Phase 3 (fingerprinting)
<!-- NEXT_PROMPT_HINT:
Phase=3
Goal=Implement fingerprint builder + similarity/diff; stable layer-wise vectors and compare CLI
ArtifactsToModify=fingerprint/builder.py,fingerprint/similarity.py,cli/fingerprint.py,cli/compare.py,tests/test_fingerprint.py
-->

# Phase 3 — Fingerprint Engine + Similarity/Diff
## What I built today
- Implemented fingerprint builder (lat_norm, ai, occ, warp_eff, l2_hit, dram_norm)
- Added cosine similarity + alignment-by-signature and diff reporting
- Exposed CLIs: `neurolens fingerprint` and `neurolens compare`
- Wrote tests that operate on sample/in-memory runs (no internet/GPU required)

## Decisions & Rationale
- Stable op signature = hash(op_type, shape, index) for cross-run alignment
- Missing counters default to 0.0 for math, but preserved as null in raw fields
- Hardware peaks optional; enable via `--peaks` for normalization

## Gaps / Open Questions
- When to auto-detect peaks from GPU model/driver (Phase 4 or 5?)
- Consider weighting vectors by latency share for similarity

## Next Steps (Phase 4)
- Build Insights/Rules engine to turn vectors+counters into actionable guidance
- Generate HTML/Markdown report with ranked suggestions
<!-- NEXT_PROMPT_HINT:
Phase=4
Goal=Implement rule-based insights engine + report generator (HTML/MD) driven by thresholds and fingerprint features
ArtifactsToModify=insights/rules.yaml,insights/engine.py,cli/report.py,tests/test_insights.py
-->

# Phase 4 — Insights Engine + Reports
## What I built today
- Implemented rule-based insights with small DSL (per-op + global)
- Scoring & ranking by latency share / overhead share
- CLI `neurolens report` renders HTML/Markdown via Jinja2 templates
- Unit tests for rule triggers + report rendering (offline)

## Decisions & Rationale
- Treat missing counters as null for logic, 0.0 for math fallbacks
- Severity-weighted scoring to prioritize time-impactful findings
- Support either run.json or fingerprint inputs

## Gaps / Open Questions
- Tune thresholds for different hardware; consider rule profiles per GPU family
- Add more diff-aware rules using fingerprint deltas

## Next Steps (Phase 5)
- Build local visualization (Dash/Streamlit): timeline, roofline, diff
<!-- NEXT_PROMPT_HINT:
Phase=5
Goal=Local visualization app + CLI launcher (`neurolens view`) with timeline, roofline, and compare pages (offline)
ArtifactsToModify=viz/dashboard.py,viz/roofline.py,cli/view.py,tests/test_viz_smoke.py
-->

# Phase 5 — Local Visualization App
## What I built today
- Added Streamlit dashboard skeleton with run viewer, diff, and roofline tabs
- Implemented roofline/timeline data helpers and fingerprint diff dataset utilities
- Introduced `neurolens view` CLI with embedded mode for tests and subprocess launcher for users
- Wrote visualization smoke/math tests and documented usage in `docs/visualization_guide.md`

## Decisions & Rationale
- Streamlit chosen for its batteries-included charts and offline friendliness
- Vega-Lite charts keep dependencies minimal while rendering timelines and roofline scatter plots
- Optional `viz` extra exposes Streamlit install without bloating core requirements

## Gaps / Open Questions
- Need richer GPU metadata to populate accurate peak compute/bandwidth ceilings
- Consider exporting static PNGs from the dashboard for sharing in reports

## Next Steps (Phase 6)
- Build export engine to bundle runs/fingerprints/reports into static artifacts
- CLI `neurolens export` to generate HTML/JSON/ZIP bundles for CI pipelines
<!-- NEXT_PROMPT_HINT:
Phase=6
Goal=Build export engine to bundle runs/fingerprints/reports into static artifacts (HTML/JSON/ZIP)
ArtifactsToModify=exporter/bundle.py,cli/export.py,tests/test_export_bundle.py
-->
# Phase 6 — Export Engine & Bundling
## What I built today
- Added `exporter/bundle.py` to package runs, fingerprints, and reports into a ZIP archive with a manifest.
- Implemented CLI `neurolens export` with options for run, fingerprint, report(s), and output directory.
- Added `docs/export_guide.md` with usage examples and manifest details.
- Wrote tests to validate archive contents, manifest correctness, and CLI behavior.

## Decisions & Rationale
- Chose ZIP for portability and ease of use.
- Manifest includes SHA-256 hashes and sizes to detect corruption.
- CLI uses Typer for consistency with other commands.

## Gaps / Open Questions
- Could add support for JSON-only bundles (without HTML) for headless environments.
- In future phases, consider optional compression levels or alternative formats (tar.gz).

## Next Steps (Phase 7)
- Build the Bench Harness & Matrix Sweeps: automate profiling across batch size, sequence length, and precision grids.
- Capture environment details (driver, CUDA version) and produce summary tables.
- Add CLI `neurolens bench` and tests for grid execution.
<!-- NEXT_PROMPT_HINT:
Phase=7
Goal=Implement bench harness and matrix sweeps; add CLI `neurolens bench` with config YAML and environment capture; write tests.
ArtifactsToModify=bench/run_matrix.py, bench/env.py, cli/bench.py, tests/test_bench.py
-->
# Phase 7 — Bench Harness & Matrix Sweeps
## What I built today
- Implemented grid runner to sweep batch_size, seq_len, precision across a model/backend.
- Captured environment info (driver, CUDA, GPU, OS, Python) once and attached to each run.
- Wrote per-run artifacts (run.json + fingerprint) and a CSV summary.
- Added CLI `neurolens bench`, tests (env + matrix), and docs (`bench_guide.md`).

## Decisions & Rationale
- Declarative YAML grid for clarity and CI friendliness.
- Schema validation enforced for each run before writing.
- Graceful mocking in tests to keep everything offline-capable.

## Gaps / Open Questions
- Consider warm-up iterations and median/p95 capture.
- Optionally parallelize runs; cap concurrency to avoid GPU thrash.

## Next Steps (Phase 8)
- Implement append-only Parquet store with manifest and optional cloud sync.
- Add CLI `neurolens ls` for fast queries/filtering.
<!-- NEXT_PROMPT_HINT:
Phase=8
Goal=Parquet run store + search CLI + optional S3/GCS sync (manifest-based)
ArtifactsToModify=storage/store.py,cli/ls.py,tests/test_store.py
-->
# Phase 8 — Scale & Storage (Parquet Store + `ls`)
## What I built today
- Added append-only storage with date/backend partitions, manifest, and optional Parquet index.
- Implemented `neurolens ls` CLI for filtering runs by backend/model/day/tags with JSON output support.
- Documented storage workflow (`docs/storage_guide.md`) and provided sample config + tests for write/query flows.

## Decisions & Rationale
- Manifest remains the source of truth; indexes are regenerated to avoid corruption drift.
- Tags are serialized for lightweight filtering without requiring nested schema support.
- CLI reuses existing Typer patterns to stay consistent with prior commands.

## Gaps / Open Questions
- Should tag matching support partial (startswith) queries?
- Consider exposing delete/expire operations while retaining append-only guarantees via tombstones.

## Next Steps (Phase 9)
- Release polish: version bump, README quickstart refresh, CLI help audit, and publish to TestPyPI.
- Add CI matrix for Python 3.10–3.12 with packaging smoke tests.
- Draft CHANGELOG and prep for first tagged release.
<!-- NEXT_PROMPT_HINT:
Phase=9
Goal=Release polish (versioning, CLI help audit, README Quickstart, TestPyPI publish, CI matrix for py3.10–3.12)
ArtifactsToModify=pyproject.toml,README.md,docs/quickstart.md,.github/workflows/ci.yml,CHANGELOG.md
-->
# Phase 9 — Packaging, Docs & CI
## What I built today
- Added release-ready metadata and console scripts to `pyproject.toml` (version `0.1.0`).
- Created GitHub Actions CI (`.github/workflows/ci.yml`) covering lint + tests on Python 3.10–3.12.
- Authored quickstart and cookbook docs; refreshed README with badges, install steps, and CLI matrix.
- Polished CLI help/ingest flow so `neurolens` lists profile, ingest, fingerprint, compare, report, ls, bench, export, and view.

## Decisions & Rationale
- Initial public version set to `0.1.0` to capture the first end-to-end toolchain release.
- CI relies on Ruff for fast linting alongside pytest to maintain code quality across versions.
- Extra docs keep README concise while providing onboarding detail elsewhere.

## Gaps / Open Questions
- Need to test installation from TestPyPI when publishing dry-run builds.
- Considering Docker images for reproducible adapter dependencies.

## Next Steps (Phase 10)
- Perform final QA across fresh environments and freeze docs.
- Exercise every CLI end-to-end, update CHANGELOG, and prep TestPyPI + PyPI releases.
- Draft release notes and confirm badges/reference links before tagging.
<!-- NEXT_PROMPT_HINT:
Phase=10
Goal=Final QA + Docs Freeze: run all commands end-to-end, fix edge cases, finalize docs & CHANGELOG, prep PyPI publish
ArtifactsToModify=tests/*, README.md, docs/*.md, pyproject.toml, CHANGELOG.md
-->
