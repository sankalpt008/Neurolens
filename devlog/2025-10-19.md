# Phase 0 — Groundwork & Spec

## What I built today
- `docs/spec.md` — Defined the Phase 0 product requirements and success metrics.
- `docs/metrics_glossary.md` — Catalogued core GPU performance counters with units and dependencies.
- `docs/architecture_overview.md` — Sketched the target architecture and module plan.
- `schema/run.schema.json` — Authored strict JSON Schema for profiling runs.
- `samples/trace_minimal.json` — Crafted a minimal schema-compliant run trace.
- `tests/test_schema_validation.py` — Added pytest coverage for valid and invalid traces.
- `jsonschema/` — Shipped a lightweight offline validator to enforce the schema without external installs.
- `pyproject.toml` — Configured packaging metadata and dev dependencies.
- `README.md` — Documented setup, testing, and validation instructions.
- `LICENSE` & `.gitignore` — Added licensing and repository hygiene files.

## Decisions & Rationale
- Enforced `additionalProperties: false` to guarantee adapters stay aligned with the schema.
- Chose enumerations for `software.backend` and `precision` to constrain early integrations.
- Required kernel-level occupancy and memory counters to support arithmetic intensity and bottleneck analysis later.
- Stored sample trace in JSON (vs Parquet) to keep onboarding lightweight while planning Parquet parity.
- Embedded a slim validator to sidestep network installs while keeping the API identical to `jsonschema`.

## Gaps / Open Questions
- Need confirmation on preferred fingerprinting algorithm (hash vs. statistical signature).
- Determine whether percentile metrics should be mandatory once histogram data is available.
- Clarify if multi-GPU runs require per-device breakdowns in Phase 1 or later.

## Next Steps (for Phase 1)
- Implement ONNX Runtime adapter to emit per-op latency timelines into `core/profiler.py`.
- Add CLI entrypoint (`cli/profile.py`) that executes a model under ORT with profiling enabled.
- Ensure the adapter writes `run.json` artifacts that pass `schema/run.schema.json` validation.
- Introduce regression tests covering multiple ops and verifying summary aggregation logic.

<!-- NEXT_PROMPT_HINT:
Phase=1
Goal=Implement ONNX Runtime per-op latency profiler and emit run.json validating against schema/run.schema.json
ArtifactsToModify=core/profiler.py, cli/profile.py
-->

# Phase 1 — Core Profiler Engine (ONNX Runtime)
## What I built today
- Implemented per-op ONNX Runtime adapter → `adapters/onnxrt_adapter.py`
- Integrated `core/profiler.py` for orchestration + schema validation
- Added CLI entrypoint `cli/profile.py` using Typer
- Added tests for schema compliance and summary logic

## Decisions & Rationale
- Stubbed kernel metrics with zeros while enforcing schema-required fields for future GPU integrations
- Allowed dependency injection for ONNX Runtime sessions to keep tests hermetic without network installs
- Validated artifacts before writing to disk to avoid leaking broken traces downstream

## Gaps / Open Questions
- How to integrate Nsight/CUPTI in Phase 2
- Whether to parallelize runs via asyncio

## Next Steps (Phase 2)
- Implement `torch_adapter.py` and `tensorrt_adapter.py`
- Add unified `Adapter` interface
- Build golden-trace comparison tests
<!-- NEXT_PROMPT_HINT:
Phase=2
Goal=Add multi-backend adapters (PyTorch, TensorRT) with uniform schema + comparison tests
ArtifactsToModify=adapters/torch_adapter.py, adapters/tensorrt_adapter.py, tests/test_adapters.py
-->
