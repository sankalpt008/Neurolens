# Phase 0 — Groundwork & Spec

## What I built today
- `docs/spec.md` — Defined the Phase 0 product requirements and success metrics.
- `docs/metrics_glossary.md` — Catalogued core GPU performance counters with units and dependencies.
- `docs/architecture_overview.md` — Sketched the target architecture and module plan.
- `schema/run.schema.json` — Authored strict JSON Schema for profiling runs.
- `samples/trace_minimal.json` — Crafted a minimal schema-compliant run trace.
- `tests/test_schema_validation.py` — Added pytest coverage for valid and invalid traces.
- `jsonschema/` — Shipped a lightweight offline validator to enforce the schema without external installs.
- `pyproject.toml` — Configured packaging metadata and dev dependencies.
- `README.md` — Documented setup, testing, and validation instructions.
- `LICENSE` & `.gitignore` — Added licensing and repository hygiene files.

## Decisions & Rationale
- Enforced `additionalProperties: false` to guarantee adapters stay aligned with the schema.
- Chose enumerations for `software.backend` and `precision` to constrain early integrations.
- Required kernel-level occupancy and memory counters to support arithmetic intensity and bottleneck analysis later.
- Stored sample trace in JSON (vs Parquet) to keep onboarding lightweight while planning Parquet parity.
- Embedded a slim validator to sidestep network installs while keeping the API identical to `jsonschema`.

## Gaps / Open Questions
- Need confirmation on preferred fingerprinting algorithm (hash vs. statistical signature).
- Determine whether percentile metrics should be mandatory once histogram data is available.
- Clarify if multi-GPU runs require per-device breakdowns in Phase 1 or later.

## Next Steps (for Phase 1)
- Implement ONNX Runtime adapter to emit per-op latency timelines into `core/profiler.py`.
- Add CLI entrypoint (`cli/profile.py`) that executes a model under ORT with profiling enabled.
- Ensure the adapter writes `run.json` artifacts that pass `schema/run.schema.json` validation.
- Introduce regression tests covering multiple ops and verifying summary aggregation logic.

<!-- NEXT_PROMPT_HINT:
Phase=1
Goal=Implement ONNX Runtime per-op latency profiler and emit run.json validating against schema/run.schema.json
ArtifactsToModify=core/profiler.py, cli/profile.py
-->

# Phase 1 — Core Profiler Engine (ONNX Runtime)
## What I built today
- Implemented per-op ONNX Runtime adapter → `adapters/onnxrt_adapter.py`
- Integrated `core/profiler.py` for orchestration + schema validation
- Added CLI entrypoint `cli/profile.py` using Typer
- Added tests for schema compliance and summary logic

## Decisions & Rationale
- Stubbed kernel metrics with zeros while enforcing schema-required fields for future GPU integrations
- Allowed dependency injection for ONNX Runtime sessions to keep tests hermetic without network installs
- Validated artifacts before writing to disk to avoid leaking broken traces downstream

## Gaps / Open Questions
- How to integrate Nsight/CUPTI in Phase 2
- Whether to parallelize runs via asyncio

## Next Steps (Phase 2)
- Implement `torch_adapter.py` and `tensorrt_adapter.py`
- Add unified `Adapter` interface
- Build golden-trace comparison tests
<!-- NEXT_PROMPT_HINT:
Phase=2
Goal=Add multi-backend adapters (PyTorch, TensorRT) with uniform schema + comparison tests
ArtifactsToModify=adapters/torch_adapter.py, adapters/tensorrt_adapter.py, tests/test_adapters.py
-->

# Phase 2 — Multi-backend Adapters (PyTorch + TensorRT)
## What I built today
- Introduced `Adapter` interface and registry
- Implemented `torch_adapter.py` with PyTorch Profiler per-op timings
- Implemented `tensorrt_adapter.py` with graceful stub fallback
- Extended `core/profiler.py` and `cli/profile.py` to select backends
- Added schema parity + golden-trace tests; conditional skips

## Decisions & Rationale
- Unified schema across backends to keep fingerprints comparable
- Stubbed TRT to unblock users without local installation
- Kept counters minimal (latency only) to de-risk before CUPTI/Nsight integration

## Gaps / Open Questions
- How to map PyTorch op names to high-level types consistently
- When to add kernel-level counters (Phase 4+)

## Next Steps (Phase 3)
- Build fingerprint engine: hardware-normalized per-layer vectors
- Implement similarity + diff across runs
- Add CLI: `neurolens fingerprint` and `neurolens compare`
<!-- NEXT_PROMPT_HINT:
Phase=3
Goal=Implement fingerprint builder + similarity/diff; generate stable layer-wise performance fingerprints and a compare CLI
ArtifactsToModify=fingerprint/builder.py,fingerprint/similarity.py,cli/fingerprint.py,cli/compare.py,tests/test_fingerprint.py
-->

# Hotfix — Remove binaries and generate models on the fly
## What I changed
- Removed committed .onnx/.pt files and added ignore rules
- Added tools/gen_add_onnx.py and tools/gen_tiny_linear.py
- Tests now generate models into tmp_path and skip gracefully when deps missing
## Why
- Codex PRs cannot include binary files; reproducible generation keeps PRs lightweight and CI-friendly
## Next
- Proceed with Phase 2 PR; then start Phase 3 (fingerprinting)
<!-- NEXT_PROMPT_HINT:
Phase=3
Goal=Implement fingerprint builder + similarity/diff; stable layer-wise vectors and compare CLI
ArtifactsToModify=fingerprint/builder.py,fingerprint/similarity.py,cli/fingerprint.py,cli/compare.py,tests/test_fingerprint.py
-->

# Phase 3 — Fingerprint Engine + Similarity/Diff
## What I built today
- Implemented fingerprint builder (lat_norm, ai, occ, warp_eff, l2_hit, dram_norm)
- Added cosine similarity + alignment-by-signature and diff reporting
- Exposed CLIs: `neurolens fingerprint` and `neurolens compare`
- Wrote tests that operate on sample/in-memory runs (no internet/GPU required)

## Decisions & Rationale
- Stable op signature = hash(op_type, shape, index) for cross-run alignment
- Missing counters default to 0.0 for math, but preserved as null in raw fields
- Hardware peaks optional; enable via `--peaks` for normalization

## Gaps / Open Questions
- When to auto-detect peaks from GPU model/driver (Phase 4 or 5?)
- Consider weighting vectors by latency share for similarity

## Next Steps (Phase 4)
- Build Insights/Rules engine to turn vectors+counters into actionable guidance
- Generate HTML/Markdown report with ranked suggestions
<!-- NEXT_PROMPT_HINT:
Phase=4
Goal=Implement rule-based insights engine + report generator (HTML/MD) driven by thresholds and fingerprint features
ArtifactsToModify=insights/rules.yaml,insights/engine.py,cli/report.py,tests/test_insights.py
-->
